\section{Theory}

\subsection{High Performance Parallel Computing}

The xTB program uses a specified variant of the GFN-xTB algorithm to compute various energies for a molecule. By default the program is set to use GFN2-xTB, which is also the variant this project focuses on.

For a molecule on the smaller scale such as a caffeine molecule which has 24 atoms, the time it takes to run xTB is perceivably fairly instantanious. Even a fullerene with 200 carbon atoms takes just about 2 seconds to compute on average on a 12th gen intel mobile processor. The computional time for even small molecules begins to be noticable, when the problem size grows to thousands or millions of molecules.

This project aims to compute the energies of fullerenes in the isomerspaces \(C_{20}, \cdots, C_{200} \), and for this purpose the computational time needed for individual molecules is of less importance. What is truly interesting for this problem domain is speeding up largely concurrent xTB computations by running them in parallel on general purpose graphic processing units (GPGPUs).

The highest level of parallelization here is to compute all the energy terms of a molecule in the same kernel. There are no data dependencies between the computations of multiple molecules, and this makes it a perfect case for massive parallelization by distributing these isolated workloads across the thousands of threads supported on modern GPGPUs.
Within the area of computing, the idea of running the same operations in parallel is known as a lockstep system. With a focus on fullerenes, which consists exclusively of carbon atoms, this type of lockstep parallelization is exactly what we want to create a fast and constant flow of data for the broader pipeline that this project is part of.

Since the executions of the xTB algorithm on each fullerene are completely isolated workloads, this means that the level of parallelization for a given isomer group, such as C20, scales with the amount of isomers in that group. This means that a much larger isomer group like C200 will also have a much greater level of parallelization.

The streaming multiprocessors (SMs) on a GPU are slower and simpler than the cores on a CPU. They have no branch prediction or other smart optimization techniques, but instead an SM has more threads it can execute in parallel in comparison to a CPU core which can only execute threads concurrently. The difference is that SMs can truly run its threads simultaniously, while CPU cores rely on context switching to make it seem like proccesses are running simultaniously. With SMs, working only on a few fullerenes will have a massive overhead from spinning up a kernel and copying data from the host(CPU) to the device(GPU), but the problem size for this project makes SMs a great fit.

The current Fortran implementation of the xTB program only takes a single molecule at a time, but when doing lockstep parallization it would be interesting to have an implementation that takes multiple molecules. This would avoid the overhead of starting multiple processes, and the program will have the data for all molecules, which gives opportunity for data coalescing by aligning the data as a structure of arrays (SOA) instead of and array of structures (AOS). It can also make copying data from the host to device more efficient since the data for multiple molecules can be moved together by the same instruction. To allow for coalesced access to the data on the device, we can essentially realign the data so that parts of the molecules that are accessed together, are also close together in memory. Doing this should also decrease the amount of copies needed between host and device memory, resulting in overall faster execution.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
squarednode/.style={
    rectangle,
    draw=black,
    fill=violet,
    text=white,
    thin,
    minimum size=5mm,
    inner sep=0pt, % No inner spacing
    outer sep=0pt, % No outer spacing
},
]

\node[squarednode] (A1) at (0,0) {$A_1$};
\node[squarednode, anchor=west] (A2) at (A1.east)   {$A_2$};
\node[squarednode, anchor=west] (A3) at (A2.east)   {$A_3$};
\node[squarednode, anchor=west] (A4) at (A3.east)   {$A_4$};

\node[squarednode, anchor=west, fill=cyan, text=black] (A1-2) at (A4.east)   {$A_1$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A2-2) at (A1-2.east)   {$A_2$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A3-2) at (A2-2.east)   {$A_3$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A4-2) at (A3-2.east)   {$A_4$};

\node[squarednode, anchor=west, fill=yellow, text=black] (A1-3) at (A4-2.east)   {$A_1$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A2-3) at (A1-3.east)   {$A_2$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A3-3) at (A2-3.east)   {$A_3$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A4-3) at (A3-3.east)   {$A_4$};



\node[squarednode, anchor=west] (A1-4) [below=of A1-2] {$A_1$};
\node[squarednode, anchor=west] (A2-4) at (A1-4.east)   {$A_2$};
\node[squarednode, anchor=west] (A3-4) at (A2-4.east)   {$A_3$};
\node[squarednode, anchor=west] (A4-4) at (A3-4.east)   {$A_4$};

\node[squarednode, anchor=west, fill=cyan, text=black] (A1-5) [below=0pt of A1-4] {$A_1$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A2-5) at (A1-5.east)   {$A_2$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A3-5) at (A2-5.east)   {$A_3$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A4-5) at (A3-5.east)   {$A_4$};

\node[squarednode, anchor=west, fill=yellow, text=black] (A1-6) [below=0pt of A1-5] {$A_1$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A2-6) at (A1-6.east)   {$A_2$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A3-6) at (A2-6.east)   {$A_3$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A4-6) at (A3-6.east)   {$A_4$};

\draw[->, thick] ($(A2-2.south) + (0.25,-0.25)$) -- ($(A2-4.north) + (0.25,0.25)$);

\node[draw=red, very thick, fit=(A1-4)(A1-5)(A1-6), inner sep=1pt] {};

\node[draw=red, very thick, fit=(A1)(A2)(A3)(A4), inner sep=1pt] {};
\end{tikzpicture}
\caption{An example of aligning molecules in memory to allow for coalesced access of the data.}
\label{fig:coalescing}
\end{figure}

An example of turning an AOS into a SOA can be seen in \autoref{fig:coalescing} where the data is realigned to allow for coalesced data access. If the code accesses only a few atoms of a molecule at a time, then it is a waste if the memory page contains the rest of a molecule as well. Instead of copying whole molecules at a time, the data should be structured such that only the required data of a molecule is copied as it allows the kernel to get that data for more molecules at a time, thus reducing unnecessary copies and speeding up the computation. 

The types of memory typically found in the memory hiearchy on a GPU are global, shared, local, and register memory. Using these levels of memory properly is crucial for achieving high performance in parallel computing tasks. Here is an overview of the various levels:

\begin{itemize}
  \item Global - Accessible from all threads on the GPU. This is the largest but also the slowest pool of memory.
  \item Shared - Tied to a thread block (or workgroup), so it can be accessed by the threads in the same thread block. This pool of memory is smaller but faster than global memory.
  \item Register Memory - Each thread on a GPU has private access to a number of registers. This is the fastest type of memory used to store local variables and intermediate results.
  \item Local Memory - Also private to each thread. This type is slower than register memory and is usually used when there is insufficient space for variables on the registers of a thread.
\end{itemize}


The NVIDIA Ada GPU architecture\cite{nvidia-ada-tuning-guide} has 64,000 32-bit registers per SM. This gives us 256 KB of register memory.

\begin{equation}
\begin{split}
  64000 \cdot 32 &= 2048000 \text{ bits}\\
  &= 256000 \text{ bytes}\\
  &= 256 \text{ KB}
\end{split}
\end{equation}

Each thread has a maximum of 255 registers, and each SM has a maximum of 24 thread blocks. An SM has a maximum of 48 warps, so full utilization can be achieved when each thread block has 2 warps allocated. A warp has 32 threads, so an SM has a total of 1536 threads.
Distributing the 64,000 registers evenly over these threads gives each thread 41 registers, which is considerably lower than the maximum of 255. This configuration allows utilizing 251.9 KB of the register memory available to the SMs.

\begin{equation}
\begin{split}
  24 \cdot 2 \cdot 32 \cdot 41 \cdot 32 &= 2015232 \text{ bits}\\
  &= 251904 \text{ bytes}\\
  &= 251.904 \text{ KB}
\end{split}
\end{equation}

This gives each thread in an SM 164 bytes of register memory to work with.

\begin{equation}
\begin{split}
  \frac{251904}{1536} &= 164 \text{ bytes}
\end{split}
\end{equation}

The shared memory capacity per SM is 100 KB and a single thread block can have a maximum of 99 KB of this. This gives each thread block about 4.16 KB of shared memory, meaning that the 64 threads in a thread block will have 65 bytes each.








83GB for 10000 C200
8.3 MB for each molecule
SMs has max 48 warps
each warp has 32 threads
64000 32bit registers per SM
each thread can max use 255 registers
max thread blocks per SM is 24
two thread blocks to make use of all 48 warps
shared memory capacity per SM 100KB
maximum shared memory per thread block is 99KB
SMs schedule warps/workgroups/thread blocks
we can do ~1536 molecules per SM with this amount of data




Doing more fine grained parallelization can make use of SIMT(Single instruction multiple threads) like reduction.
