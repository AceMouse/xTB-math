\section{Theory}

\subsection{High Performance Parallel Computing}

The xTB program uses a specified variant of the GFN-xTB algorithm to compute various energies for a molecule. By default the program is set to use GFN2-xTB, which is also the variant this project focuses on.

For a molecule on the smaller scale such as a caffeine molecule which has 24 atoms, the time it takes to run xTB is fairly instantanious. Even a fullerene with 200 carbon atoms takes just about 2 seconds to compute on average on a 12th gen intel mobile processor. The computional time for even small molecules begins to be noticable, when the problem size grows to thousands or millions of molecules.

This project aims to compute the energies of fullerenes in the isomerspaces \(C_{20}, \cdots, C_{200} \), and for this purpose the computational time needed for individual molecules is of less importance. What is truly interesting for this problem domain is speeding up largely concurrent xTB computations by running them in parallel on general purpose graphic processing units (GPGPUs).

The highest level of parallelization here is to compute all the energy terms of a molecule in the same kernel. There are no data dependencies between the computations of multiple molecules, and this makes it a perfect case for massive parallelization by distributing these isolated workloads across the thousands of threads supported on modern GPGPUs.
Within the area of computing, the idea of running the same operations in parallel is known as a lockstep system. With a focus on fullerenes, which consists exclusively of carbon atoms, this type of lockstep parallelization is exactly what we want to create a fast and constant flow of data for the broader pipeline that this project is part of.

Since the executions of the xTB algorithm on each fullerene are completely isolated workloads, this means that the level of parallelization for a given isomer group, such as C20, scales with the amount of isomers in that group. This means that a much larger isomer group like C200 will also have a much greater level of parallelization.

The streaming multiprocessors (SMs) on a GPU are slower and simpler than the cores on a CPU. They have no branch prediction or other smart optimization techniques, but instead an SM has more threads it can execute in parallel in comparison to a CPU core which can only execute threads concurrently. The difference is that SMs can truly run its threads simultaniously, while CPU cores rely on context switching to make it seem like proccesses are running simultaniously. With SMs, working only on a few fullerenes will have a massive overhead from spinning up a kernel and copying data from the host(CPU) to the device(GPU), but the problem size for this project makes SMs a great fit.

The current Fortran implementation of the xTB program only takes a single molecule at a time, but when doing lockstep parallization it would be interesting to have an implementation that takes multiple molecules. This would avoid the overhead of starting multiple processes, and the program will have the data for all molecules, which gives opportunity for data coalescing by aligning the data as a structure of arrays (SOA) instead of and array of structures (AOS). It can also make copying data from the host to device more efficient since the data for multiple molecules can be moved together by the same instruction. To allow for coalesced access to the data on the device, we can essentially realign the data so that parts of the molecules that are accessed together, are also close together in memory. Doing this should also decrease the amount of copies needed between host and device memory, resulting in overall faster execution.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
squarednode/.style={
    rectangle,
    draw=black,
    fill=violet,
    text=white,
    thin,
    minimum size=5mm,
    inner sep=0pt, % No inner spacing
    outer sep=0pt, % No outer spacing
},
]

\node[squarednode] (A1) at (0,0) {$A_1$};
\node[squarednode, anchor=west] (A2) at (A1.east)   {$A_2$};
\node[squarednode, anchor=west] (A3) at (A2.east)   {$A_3$};
\node[squarednode, anchor=west] (A4) at (A3.east)   {$A_4$};

\node[squarednode, anchor=west, fill=cyan, text=black] (A1-2) at (A4.east)   {$A_1$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A2-2) at (A1-2.east)   {$A_2$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A3-2) at (A2-2.east)   {$A_3$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A4-2) at (A3-2.east)   {$A_4$};

\node[squarednode, anchor=west, fill=yellow, text=black] (A1-3) at (A4-2.east)   {$A_1$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A2-3) at (A1-3.east)   {$A_2$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A3-3) at (A2-3.east)   {$A_3$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A4-3) at (A3-3.east)   {$A_4$};



\node[squarednode, anchor=west] (A1-4) [below=of A1-2] {$A_1$};
\node[squarednode, anchor=west] (A2-4) at (A1-4.east)   {$A_2$};
\node[squarednode, anchor=west] (A3-4) at (A2-4.east)   {$A_3$};
\node[squarednode, anchor=west] (A4-4) at (A3-4.east)   {$A_4$};

\node[squarednode, anchor=west, fill=cyan, text=black] (A1-5) [below=0pt of A1-4] {$A_1$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A2-5) at (A1-5.east)   {$A_2$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A3-5) at (A2-5.east)   {$A_3$};
\node[squarednode, anchor=west, fill=cyan, text=black] (A4-5) at (A3-5.east)   {$A_4$};

\node[squarednode, anchor=west, fill=yellow, text=black] (A1-6) [below=0pt of A1-5] {$A_1$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A2-6) at (A1-6.east)   {$A_2$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A3-6) at (A2-6.east)   {$A_3$};
\node[squarednode, anchor=west, fill=yellow, text=black] (A4-6) at (A3-6.east)   {$A_4$};

\draw[->, thick] ($(A2-2.south) + (0.25,-0.25)$) -- ($(A2-4.north) + (0.25,0.25)$);

\node[draw=red, very thick, fit=(A1-4)(A1-5)(A1-6), inner sep=1pt] {};

\node[draw=red, very thick, fit=(A1)(A2)(A3)(A4), inner sep=1pt] {};
\end{tikzpicture}
\caption{This figure transforms an array of structures where the memory for each structure is laid out contiguously, into a structure of arrays where data relevant for a computation is close together. This way of grouping related data close together is known as spatial locality and allows for coalesced access of the data.}
\label{fig:coalescing}
\end{figure}

An example of turning an AOS into a SOA can be seen in \autoref{fig:coalescing} where the data is realigned to allow for coalesced data access. If the code accesses only a few atoms of a molecule at a time, then it is a waste if the memory page contains the rest of a molecule as well. Instead of copying whole molecules at a time, the data should be structured such that only the required data of a molecule is copied as it allows the kernel to get that data for more molecules at a time, thus reducing unnecessary copies and speeding up the computation. 


\subsection{Memory Types and Current Hardware}

The types of memory typically found in the memory hiearchy on a GPU are global, shared, local, and register memory. Using these levels of memory properly is crucial for achieving high performance in parallel computing tasks. Here is an overview of the various levels:

\begin{itemize}
  \item Global - Accessible from all threads on the GPU. This is the largest but also the slowest pool of memory.
  \item Shared - Tied to a thread block (or workgroup), so it can be accessed by the threads in the same thread block. This pool of memory is smaller but faster than global memory.
  \item Register Memory - Each thread on a GPU has private access to a number of registers. This is the fastest type of memory used to store local variables and intermediate results.
  \item Local Memory - Also private to each thread. This type is slower than register memory and is usually used when there is insufficient space for variables on the registers of a thread.
\end{itemize}


The NVIDIA Ada Lovelace GPU architecture\cite{nvidia-ada-tuning-guide} has $80$ GB of global memory, $100$ KB of shared memory per SM, and $64,000$ $32$-bit registers per SM, which gives us $256$ KB of register memory per SM.

\begin{equation}
\begin{split}
  64000 \cdot 32 &= 2048000 \text{ bits}\\
  &= 256000 \text{ bytes}\\
  &= 256 \text{ KB}
\end{split}
\end{equation}

Each thread has a maximum of $255$ registers, and each SM has a maximum of $24$ blocks and a maximum of $48$ warps. This adds up such that full utilization can be achieved when each block has $2$ warps allocated. A warp has $32$ threads, so an SM has a total of $1536$ threads when each of its $24$ blocks has two warps.
Distributing the $64,000$ registers evenly over these threads gives each of them $41$ registers, which is considerably lower than the maximum of $255$, but allows all of the threads to be used evenly. By using \autoref{eq:reg_mem_utilized_per_sm} we can see that this configuration utilizes $251.9$ KB of the register memory available to each of the SMs.

\begin{equation}
\begin{split}
  WarpsPerBlock &= \floor*{\frac{WarpsPerSM}{BlocksPerSM}}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
  ThreadsPerBlock &= WarpsPerBlock \times ThreadsPerWarp
\end{split}
\end{equation}

\begin{equation}
\begin{split}
  RegistersPerThread &= \floor*{\frac{RegistersPerSM}{BlocksPerSM \times ThreadsPerBlock}}
\end{split}
\end{equation}

\begin{equation} \label{eq:reg_mem_utilized_per_sm}
\begin{split}
  RegisterMemoryUtilizedPerSM =\\
  &BlocksPerSM\\
  &\times ThreadsPerBlock\\
  &\times RegistersPerThread\\
  &\times BitsPerRegister
\end{split}
\end{equation}

BlocksPerSM and BitsPerRegister are constants found in the specification for the GPU architecture.

With this, each block in an SM has $10.496$ KB of register memory to work with, and thus each thread has $164$ bytes.

\begin{equation}
\begin{split}
  \frac{251.904}{24} &= 10.496 \text{ KB}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
  \frac{251904}{1536} &= 164 \text{ bytes}
\end{split}
\end{equation}

The shared memory capacity per SM is $100$ KB and a single block can have a maximum of $99$ KB. This gives each block about $4.16$ KB of shared memory, meaning that the $64$ threads in a block will have $65$ bytes each.

\begin{equation}
\begin{split}
  \frac{100}{24} &= 4.166 \text{ KB}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
  \frac{4166}{64} &= 65.09 \text{ bytes}
\end{split}
\end{equation}

The Ada Lovelace architecture has a total of $128$ SMs and it also has a $98304$ KB L2 cache. This results in a total of $3072$ blocks that each has $32$ KB of the L2 Cache. All of this combined gives each block a total of $46.656$ KB of non-global memory.

\begin{equation}
\begin{split}
  10.496 \text{ KB} + 4.166 \text{ KB} + 32 \text{ KB} &= 46.662 \text{ KB}
\end{split}
\end{equation}

Distributing the global memory across the blocks gives each of them $26.042$ MB. Combined with the non-global memory, this results in each block having a total of $26.088$ MB of memory.

\subsection{Optimizing GPU Configuration for Isolated Molecules}

Computing xTB-GFN2 for a molecule is completely isolated, so if we do the whole xTB computation in lockstep, then we only have to concern ourselves with how many warps and blocks a single molecule needs in order to keep all its data in device memory. When we know these details, then we can simply scale up the number of molecules to be computed in lockstep until all the resources on the GPU are saturated. With this approach, expanding to multiple GPUs should be rather trivial as there are no interdependencies.

The exact space needed to compute the GFN2 variant of xTB for a single fullerene is not yet clear to us, and it will also vary based on the size of the fullerene. To make finding the most optimal GPU configuration easier when optimizing for most possible parallel xTB computations, we have developed a script that computes such a configuration based on the space requirement of a single molecule.

\begin{figure}[h]
\begin{minted}{python}
  def compute(bytes_per_molecule):
    mb_per_molecule = bytes_per_molecule / 1_000_000

    threads_per_molecule = compute_threads_per_molecule(mb_per_molecule)
    warps_per_molecule = compute_warps_per_molecule(threads_per_molecule)
    molecules_per_block = compute_molecules_per_block(warps_per_molecule)

    print_utilization(warps_per_molecule, molecules_per_block)

    print(f"MB per molecule: {mb_per_molecule} MB")
    print(f"Threads per molecule: {threads_per_molecule}")
    print(f"Warps per molecule: {warps_per_molecule}")
    print(f"Molecules per block: {molecules_per_block}")
\end{minted}
  \caption{This is the top-level function for computing the optimal number of warps needed per molecule when optimizing for most possible parallel xTB computations on a single GPU specifically for the Ada Lovelace architecture.}
\end{figure}

The output of the script has information on how many threads are required for a single molecule, how many warps this fits within. It also includes general information about the utilization of the GPUs resources with the configuration presented. An example of the full output can be seen in \autoref{fig:gpu-mem-output}.


\begin{figure}[h]
\begin{minted}[linenos=false]{text}
| Global memory | L2 cache | Total SMs | Threads per warp |
|    80.0 GB    | 98304 KB |    128    |        32        |
| Threads Available | Threads Used | Blocks Available | Blocks Used |
|      196608       |    196608    |       3072       |    3072.0   |

#################################### Per SM ####################################
| Warps | Blocks | Registers | Shared Memory | Threads Available | Threads Used |
|  48   |   24   |   64000   |    100 KB     |       1536        |     1536     |
| Blocks Available | Blocks Used |
|        24        |     24.0    |

########################### Per Block ###########################
| L2 Cache | Register Mem | Shared Mem | Global Mem | Total mem |
| 32.0 KB  |  10.496 KB   |  4.167 KB  |  26.042 MB | 26.088 MB |

MB per molecule: 8.3 MB
Threads per molecule: 18
Warps per molecule: 1
Molecules per block: 2
\end{minted}
\caption{This is the output of our script for computing an optimal GPU configuration when a single molecule needs $8.3$ MB of memory.}
\label{fig:gpu-mem-output}
\end{figure}

In \autoref{fig:gpu-mem-output} each molecule is $8.3$ MB and thus requires the memory of $18$ threads. Each warp has $32$ threads, so in this case a single molecule fits within a single warp. With $24$ blocks and $48$ warps per SM, all blocks can have $2$ warps each, which in this scenario means that $2$ molecules can be computed in parallel within a single block. This space requirement for a molecule is just an example and in no way guaranteed to be representative of the actual requirements of fullerenes of any size.


%The electrostatic energy term depends on the initial Huckel energy term H0, P, dq, dqsh, atomicGam, shellGam, jmat\_flat, shift...
%
%Instead of giving a bad approximation then just explain the equations and how to use the script.
%
%83GB for 10000 C200
%8.3 MB for each molecule
%SMs has max 48 warps
%each warp has 32 threads
%64000 32bit registers per SM
%each thread can max use 255 registers
%max thread blocks per SM is 24
%two thread blocks to make use of all 48 warps
%shared memory capacity per SM 100KB
%maximum shared memory per thread block is 99KB
%SMs schedule warps/workgroups/thread blocks
%we can do ~1536 molecules per SM with this amount of data




%Doing more fine grained parallelization can make use of SIMT(Single instruction multiple threads) like reduction.
